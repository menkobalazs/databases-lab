{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df381bc-5714-4702-8c06-6e9ff172972a",
   "metadata": {},
   "source": [
    "# Inferring Photometric Redshifts from Multichannel Images\n",
    "## Bal치zs Menk칩 (O67UT7)\n",
    "### Supervisor: P치l, Bal치zs \n",
    "\n",
    "Spectroscopic observations of distant astronomical targets are increasingly difficult and expensive to obtain. Therefore, it is crucial to develop methods for inferring the physical parameters of objects from photometric data, which is the only type of observation available at high redshifts. The Sloan Digital Sky Survey (SDSS) data set is a valuable resource for this task, as it contains both photometric and spectroscopic data for a very large number of galaxies.\n",
    "\n",
    "Analyze the photometric and spectroscopic data of galaxies together! Download coordinates, redshifts and appropriate object identifiers from the SDSS SkyServer! Create square-shaped images from the SDSS sky survey observations that contain single galaxies in the center! Build a simple convolutional neural network that is able to infer redshifts from the cut-out images of galaxies!\n",
    "\n",
    "#### How accurate is your model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134fbe2",
   "metadata": {},
   "source": [
    "Change **Kooplex Environment** image to `image-registry.vo.elte.hu/jupyter-tensorflow-v6` and set `CPU [#]` to the maximum.\n",
    "\n",
    "Install `psycopg2`:\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "pip install psycopg2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PACKAGES\n",
    "from utils import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tf_kl\n",
    "import tensorflow.keras.regularizers as tf_kr\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31ff3b-8503-41c3-8b13-41d2dc65a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pictures = 8000\n",
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da4b076-25dc-41ee-8cf7-6a0648e47f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(137)\n",
    "indices = np.arange(0, number_of_pictures)\n",
    "np.random.shuffle(indices)\n",
    "train_set_indices = indices[:int(len(indices)*0.5)]\n",
    "test_set_indices = indices[int(len(indices)*0.5):int(len(indices)*0.8)]\n",
    "#validation_set_indices = indices[int(len(indices)*0.8):]\n",
    "\n",
    "train_set_indices = '), ('.join(map(str, list(train_set_indices)))\n",
    "test_set_indices = '), ('.join(map(str, list(test_set_indices)))\n",
    "#validation_set_indices = '), ('.join(map(str, list(validation_set_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17adcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = query_a_needed_set(train_set_indices)\n",
    "test_set = query_a_needed_set(test_set_indices)\n",
    "#validation_set = query_a_needed_set(validation_set_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column='picture'\n",
    "train_set_pictures = get_data(train_set, column)\n",
    "test_set_pictures = get_data(test_set, column)\n",
    "#validation_set_pictures = get_data(validation_set, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column='z'\n",
    "train_set_features = get_data(train_set, column)\n",
    "test_set_features = get_data(test_set, column)\n",
    "#validation_set_features = get_data(validation_set, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba372b45",
   "metadata": {},
   "source": [
    "---\n",
    "# Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters=32\n",
    "kernel_size=(3,3)\n",
    "padding='same'\n",
    "activation='relu'\n",
    "regularization=5e-5\n",
    "\n",
    "\n",
    "model_sdss = tf.keras.models.Sequential([ \n",
    "    # Input\n",
    "    tf_kl.Input(shape=(train_set_pictures.shape[1:]), name='Input_layer'),\n",
    "    \n",
    "    # Convolutional block 1 --- 3x3CONVxfltr -> ReLU -> 3x3CONVxfltr -> ReLU -> MAXPOOL2x2\n",
    "        tf_kl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-1'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-2'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-3'),\n",
    "        tf_kl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-4'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-5'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-6'),\n",
    "        tf_kl.MaxPool2D(strides=(2,2), name='MaxPool2D_layer-7'),\n",
    "    \n",
    "    # Convolutional block 2 --- 3x3CONVx2*fltr -> ReLU -> 3x3CONVx2*fltr -> ReLU -> MAXPOOL2x2\n",
    "        tf_kl.Conv2D(filters=2*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-8'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-9'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-10'),\n",
    "        tf_kl.Conv2D(filters=2*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-11'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-12'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-13'),\n",
    "        tf_kl.MaxPool2D(strides=(2,2), name='MaxPool2D_layer-14'),\n",
    "    \n",
    "    # Convolutional block 3 --- 3x3CONVx4*fltr -> ReLU -> 1x1CONVx2*fltr -> ReLU -> 3x3CONVx4*fltr -> ReLU -> MAXPOOL2x2\n",
    "        tf_kl.Conv2D(filters=4*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-15'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-16'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-17'),\n",
    "        tf_kl.Conv2D(filters=2*filters, kernel_size=(1,1), padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-18'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-19'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-20'),\n",
    "        tf_kl.Conv2D(filters=4*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-21'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-22'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-23'),\n",
    "        tf_kl.MaxPool2D(strides=(2,2), name='MaxPool2D_layer-24'),\n",
    "    \n",
    "    # Convolutional block 4 --- 3x3CONVx8*fltr -> ReLU -> 1x1CONVx4*fltr -> ReLU -> 3x3CONVx8*fltr -> ReLU -> MAXPOOL2x2\n",
    "        tf_kl.Conv2D(filters=8*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-25'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-26'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-27'),\n",
    "        tf_kl.Conv2D(filters=4*filters, kernel_size=(1,1), padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-28'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-29'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-30'),\n",
    "        tf_kl.Conv2D(filters=8*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-31'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-32'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-33'),\n",
    "        tf_kl.MaxPool2D(strides=(2,2), name='MaxPool2D_layer-34'),\n",
    "    \n",
    "    # Convolutional block 5. --- 3x3CONVx16*fltr -> ReLU -> 1x1CONVx8*fltr -> ReLU -> 3x3CONVx16*fltr -> ReLU -> AVGPOOL\n",
    "        tf_kl.Conv2D(filters=16*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-35'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-36'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-37'),\n",
    "        tf_kl.Conv2D(filters=8*filters, kernel_size=(1,1), padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-38'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-39'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-40'),\n",
    "        tf_kl.Conv2D(filters=16*filters, kernel_size=kernel_size, padding=padding, \n",
    "                     kernel_regularizer=tf_kr.l2(regularization), name='Conv2D_layer-41'),\n",
    "        tf_kl.BatchNormalization(name='BatchNorm_layer-42'),\n",
    "        tf_kl.Activation(activation, name='Relu_layer-43'),\n",
    "        tf_kl.GlobalAveragePooling2D(name='GlobAvgPool_layer-44'),  \n",
    "    \n",
    "    # End of convolution\n",
    "        tf_kl.Dense(1, name='Output_layer')],\n",
    "    \n",
    "    name=\"Model-SDSS\")\n",
    "\n",
    "model_sdss.compile(loss='mean_squared_error',\n",
    "                   metrics={'Output_layer': ['mean_absolute_error']},\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n",
    "model_sdss.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_sdss = model_sdss.fit(x = train_set_pictures,\n",
    "                              y = train_set_features, \n",
    "                              validation_data = (test_set_pictures, test_set_features),\n",
    "                              batch_size = batch_size, \n",
    "                              epochs = epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "np.save(f\"models/history-sdss_time={now.month}-{now.day}-{now.hour}-{now.minute}\"+\\\n",
    "        f\"_n-pic={number_of_pictures}_epoch={epochs}_batch-size={batch_size}.npy\", \n",
    "        history_sdss.history)\n",
    "tf.keras.models.save_model(\n",
    "    model_sdss,  \n",
    "    f\"models/model-sdss_time={now.month}-{now.day}-{now.hour}-{now.minute}\"+\\\n",
    "    f\"_n-pic={number_of_pictures}_epoch={epochs}_batch-size={batch_size}.keras\",\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e32fe0",
   "metadata": {},
   "source": [
    "---\n",
    "# Continue training\n",
    "\n",
    "### Trained models:\n",
    "First run --- redshift --- $ N_{pic}=8000$, epochs$=50$, batch size$=128$, time=11-19-0-30\n",
    "\n",
    "Second run --- redshift --- $ N_{pic}=8000$, epochs$=80$, batch size$=128$, time=11-20-19-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c8ff2-793e-402b-bc0b-bbad09ca0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_nth_model = 1  \n",
    "\n",
    "if load_nth_model == 1:\n",
    "    number_of_pictures = 8000\n",
    "    str_epochs = 50\n",
    "    str_batch_size = 128\n",
    "    str_time='11-19-0-30'\n",
    "elif load_nth_model == 2:\n",
    "    number_of_pictures = 8000\n",
    "    str_epochs = 80\n",
    "    str_batch_size = 128\n",
    "    str_time='11-20-19-26'   \n",
    "else:\n",
    "    print(f\"There is no model with parameter 'load_nth_model={load_nth_model}.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    model_sdss = tf.keras.models.load_model(\n",
    "        f\"models/model-sdss_time={str_time}\"+\\\n",
    "        f\"_n-pic={str_number_of_pictures}_epoch={str_epochs}_batch-size={str_batch_size}.keras\",\n",
    "    )\n",
    "\n",
    "    if str_number_of_pictures == number_of_pictures:\n",
    "        continue_tr_epochs=30\n",
    "        history_sdss = model_sdss.fit(x = train_set_pictures,\n",
    "                                      y = train_set_features, \n",
    "                                      validation_data = (test_set_pictures, test_set_features),\n",
    "                                      batch_size = str_batch_size, \n",
    "                                      epochs = continue_tr_epochs\n",
    "                                     )\n",
    "\n",
    "    now = datetime.now()\n",
    "    if str_number_of_pictures == number_of_pictures:\n",
    "        np.save(f\"models/history-sdss_time={now.month}-{now.day}-{now.hour}-{now.minute}\"+\\\n",
    "                f\"_n-pic={str_number_of_pictures}_epoch={str_epochs}+{continue_tr_epochs}_batch-size={batch_size}.npy\", \n",
    "                history_sdss.history)\n",
    "        tf.keras.models.save_model(\n",
    "            model_sdss,  \n",
    "            f\"models/model-sdss_time={now.month}-{now.day}-{now.hour}-{now.minute}\"+\\\n",
    "            f\"_n-pic={str_number_of_pictures}_epoch={str_epochs}+{epochs}_batch-size={batch_size}.keras\",\n",
    "            overwrite=False,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1226170",
   "metadata": {},
   "source": [
    "---\n",
    "### Run times:\n",
    "First run --- redshift --- $ N_{pic}=8000$, epochs$=50$, batch size$=128$, time=11-19-0-30\n",
    "```python\n",
    "# CPU times: user 17h 19min 31s, sys: 3h 5min 10s, total: 20h 24min 41s\n",
    "# Wall time: 4h 10min 42s\n",
    "```\n",
    "\n",
    "Second run --- redshift --- $ N_{pic}=8000$, epochs$=80$, batch size$=128$, time=11-20-19-26\n",
    "```python\n",
    "# CPU times: user 21h 57min 43s, sys: 3h 31min 45s, total: 1d 1h 29min 29s\n",
    "# Wall time: 5h 10min 41s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
